{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbht04/ai-agents/blob/main/CRM_Lead_Qualifier_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRM Lead Qualifier Agent\n",
        "\n",
        "## Goal\n",
        "\n",
        "To develop an AI agent that automatically enriches a new sales lead (identified by an email address) by gathering publicly available company information, checking for prior engagement in the internal CRM, and assigning a preliminary qualification score.\n",
        "\n",
        "## Context\n",
        "\n",
        "Sales representatives often spend valuable time manually researching leads and cross-referencing internal systems before a discovery call. This process is slow, inconsistent, and often leads to a poorly prepared first interaction.\n",
        "\n",
        "## Agent Functionality\n",
        "\n",
        "The agent must be able to:\n",
        "\n",
        "1. Extract Domain: Take the email address and extract the company domain name (e.g., jane@acmecorp.com → acmecorp.com).\n",
        "\n",
        "2. Enrich Company Data: Use the domain to look up (simulated) company details like industry, size, and annual revenue.\n",
        "\n",
        "3. Check CRM History: Search the internal (simulated) CRM for any past contact or notes associated with the lead's email.\n",
        "\n",
        "4. Calculate Lead Score: Synthesize all gathered data to assign a qualitative priority score (e.g., High, Medium, Low).\n",
        "\n",
        "5. Final Summary: Present a concise, actionable summary of all findings to the sales representative."
      ],
      "metadata": {
        "id": "_QO_trOxuwxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Technical Implementation\n",
        "\n",
        "We will use the OpenAI client's Function Calling capability to define and execute the necessary business logic tools in a structured loop.\n",
        "\n",
        "## What is function calling?\n",
        "\n",
        "Function calling is the mechanism that bridges the gap between an AI model (which is just a text generator) and your actual code (which can perform actions).\n",
        "\n",
        "Think of the Large Language Model (LLM) as a very smart receptionist. It understands what people want, but it doesn't have the keys to the file cabinet or the ability to make phone calls itself.\n",
        "\n",
        "Function calling gives the receptionist a \"menu\" of services it can request from the back office (your code).\n",
        "\n",
        "![](https://cdn.openai.com/API/docs/images/function-calling-diagram-steps.png)\n",
        "\n",
        "### The Core Concept\n",
        "\n",
        "* You provide the tools: You tell the model, \"I have a function called get_weather(city) that takes a city name as an argument.\"\n",
        "\n",
        "* The model \"thinks\": If a user asks, \"What's the weather in Tokyo?\", the model recognizes that your tool can solve this.\n",
        "\n",
        "* The model outputs JSON (not text): Instead of replying to the user, the model pauses and gives you a structured request: {\"function\": \"get_weather\", \"arguments\": {\"city\": \"Tokyo\"}}.\n",
        "\n",
        "* You execute: Your code sees this request, runs the actual Python function, and gets the result (e.g., \"Sunny, 25°C\").\n",
        "\n",
        "* The model finishes: You feed that result back to the model, and it writes the final natural language answer: \"It is currently sunny and 25 degrees in Tokyo.\"\n",
        "\n",
        "### Why is this powerful?\n",
        "\n",
        "Without function calling, LLMs are isolated text predictors—they can't \"do\" anything. With function calling, they become agents.\n",
        "\n",
        "* **Structured Data Extraction**: Instead of hoping the model formats data correctly, you force it to output clean JSON that fits your database schema.\n",
        "\n",
        "* **Connecting to the World**: It allows the AI to browse the web, query databases, send emails, or control software, merely by defining those actions as \"functions.\""
      ],
      "metadata": {
        "id": "m--Xpb4VvOri"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ-uD7t_uDZE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Initialize OpenAI Client ---\n",
        "try:\n",
        "    client = OpenAI(api_key=userdata.get('OPENAI_APIKEY'))\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing OpenAI client: {e}\")\n",
        "    print(\"Please ensure your OPENAI_API_KEY is set in your environment variables.\")"
      ],
      "metadata": {
        "id": "4ItxCvj1uIJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d7e455-4261-43e6-c355-42d13631cdfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error initializing OpenAI client: Secret OPENAI_APIKEY does not exist.\n",
            "Please ensure your OPENAI_API_KEY is set in your environment variables.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the \"Tools\" (Business Logic)\n",
        "\n",
        "Here we define the actual Python functions that perform the work. In AI Agent terminology, these are the **Tools**.\n",
        "\n",
        "For this demonstration, we are mocking the external systems (simulating database lookups with dictionaries), but in a production environment, these functions would:\n",
        "1.  **`lookup_domain_info`**: Query an API like Clearbit or Crunchbase.\n",
        "2.  **`check_crm_history`**: Query a Salesforce or HubSpot SQL database.\n",
        "3.  **`calculate_lead_score`**: Run a custom algorithm or ML model to grade the lead.\n",
        "\n",
        "The agent \"calls\" these tools by asking us to run these specific Python functions."
      ],
      "metadata": {
        "id": "d_2pADXp3Sbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def lookup_domain_info(domain: str) -> str:\n",
        "    \"\"\"\n",
        "    Looks up and returns mock company information based on its domain.\n",
        "    In a real system, this would call an external data enrichment API (e.g., Clearbit).\n",
        "    \"\"\"\n",
        "    print(f\"-> TOOL ACTIVATED: Looking up domain info for {domain}...\")\n",
        "\n",
        "    # Mock database for demonstration\n",
        "    mock_data = {\n",
        "        \"acmecorp.com\": {\"industry\": \"Software/SaaS\", \"size\": \"501-1000 employees\", \"revenue\": \"$50M - $100M\"},\n",
        "        \"widgetco.net\": {\"industry\": \"Manufacturing\", \"size\": \"100-250 employees\", \"revenue\": \"$10M - $25M\"},\n",
        "        \"globalfin.org\": {\"industry\": \"Financial Services\", \"size\": \"5000+ employees\", \"revenue\": \"$1B+\"},\n",
        "    }\n",
        "\n",
        "    info = mock_data.get(domain, {\"industry\": \"Unknown\", \"size\": \"N/A\", \"revenue\": \"N/A\"})\n",
        "\n",
        "    # Return the data as a JSON string for the AI model to parse easily\n",
        "    return json.dumps(info)"
      ],
      "metadata": {
        "id": "rNbhPwLYuLLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_crm_history(email: str) -> str:\n",
        "    \"\"\"\n",
        "    Checks the internal CRM system for past engagement history with the lead.\n",
        "    In a real system, this would query a PostgreSQL database or a CRM API (e.g., Salesforce).\n",
        "    \"\"\"\n",
        "    print(f\"-> TOOL ACTIVATED: Checking CRM history for {email}...\")\n",
        "\n",
        "    # Mock database for demonstration\n",
        "    mock_data = {\n",
        "        \"jane@acmecorp.com\": {\"last_contact\": \"2025-11-15\", \"status\": \"Cold Lead\", \"notes\": \"Attended webinar, no follow-up yet.\"},\n",
        "        \"bob@widgetco.net\": {\"last_contact\": \"2025-12-01\", \"status\": \"Active Opportunity\", \"notes\": \"Discussed Q1 budget and product integration.\"},\n",
        "        \"default\": {\"last_contact\": \"N/A\", \"status\": \"No Record\", \"notes\": \"New lead, first contact opportunity.\"},\n",
        "    }\n",
        "\n",
        "    history = mock_data.get(email, mock_data[\"default\"])\n",
        "    return json.dumps(history)\n"
      ],
      "metadata": {
        "id": "tcsvLMM_uOXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_lead_score(data_summary: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyzes the collected data (domain and CRM history) to assign a lead score (High/Medium/Low).\n",
        "    This function simulates a complex scoring algorithm.\n",
        "    \"\"\"\n",
        "    print(\"-> TOOL ACTIVATED: Calculating lead score...\")\n",
        "\n",
        "    data = json.loads(data_summary)\n",
        "    score = \"Low\" # Default score\n",
        "\n",
        "    # Simple scoring logic for demonstration\n",
        "    if data[\"domain_info\"].get(\"revenue\", \"\").startswith(\"$1B+\"):\n",
        "        score = \"High\"\n",
        "    elif data[\"crm_history\"].get(\"status\") == \"Active Opportunity\":\n",
        "        score = \"High\"\n",
        "    elif data[\"domain_info\"].get(\"revenue\", \"\").startswith(\"$50M\"):\n",
        "        score = \"Medium\"\n",
        "\n",
        "    return json.dumps({\"lead_score\": score})"
      ],
      "metadata": {
        "id": "ph8euhMOuQ0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map of available function names to the actual Python functions\n",
        "AVAILABLE_FUNCTIONS = {\n",
        "    \"lookup_domain_info\": lookup_domain_info,\n",
        "    \"check_crm_history\": check_crm_history,\n",
        "    \"calculate_lead_score\": calculate_lead_score,\n",
        "}"
      ],
      "metadata": {
        "id": "EbRbwHfUuS1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring the Agent's \"Menu\" (Tool Schema)\n",
        "\n",
        "The Large Language Model (LLM) cannot see our Python code directly. We must describe our tools to it using a specific JSON format known as a **Schema**.\n",
        "\n",
        "This schema tells the model:\n",
        "* **What** the tool does (Description).\n",
        "* **When** to use it (Context).\n",
        "* **How** to use it (Parameters/Arguments).\n",
        "\n",
        "We pass this list to the `tools` parameter in the API call later. It effectively gives the AI a \"menu\" of actions it can take."
      ],
      "metadata": {
        "id": "CbPwwLX63z1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is how the AI model learns about the tools it can use.\n",
        "tools_schema = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"lookup_domain_info\",\n",
        "            \"description\": \"Retrieves general business information (industry, size, revenue) about a company based on its domain name.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"domain\": {\"type\": \"string\", \"description\": \"The company's domain name, e.g., 'acmecorp.com'\"},\n",
        "                },\n",
        "                \"required\": [\"domain\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"check_crm_history\",\n",
        "            \"description\": \"Checks the internal CRM system for past contact, status, and notes associated with a specific lead email.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"email\": {\"type\": \"string\", \"description\": \"The full email address of the lead.\"},\n",
        "                },\n",
        "                \"required\": [\"email\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"calculate_lead_score\",\n",
        "            \"description\": \"Calculates the priority score (High/Medium/Low) for a lead based on a summary of all collected domain and CRM history data.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"data_summary\": {\"type\": \"string\", \"description\": \"A JSON string containing the combined domain_info and crm_history.\"},\n",
        "                },\n",
        "                \"required\": [\"data_summary\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "vWaYaQd4uV7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Agent Loop (Think → Act → Observe)\n",
        "\n",
        "This is the brain of the application. The `run_agent` function implements the core feedback loop required for an autonomous agent.\n",
        "\n",
        "**How it works:**\n",
        "1.  **State Management (`collected_data`)**: We initialize a dictionary *outside* the loop to act as the agent's \"memory\" across multiple steps.\n",
        "2.  **Think**: We send the conversation history to the model (`client.chat.completions.create`).\n",
        "3.  **Act**: If the model decides to call a tool, we execute the corresponding Python function.\n",
        "4.  **Observe**: We append the tool's output to the conversation history as a new message.\n",
        "5.  **Repeat**: The loop continues until the model decides it has enough information to answer the user directly.\n",
        "\n"
      ],
      "metadata": {
        "id": "tkeMXHyJ39eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_agent(user_prompt: str):\n",
        "    \"\"\"\n",
        "    The main execution loop for the CRM Lead Qualifier Agent.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Running Lead Qualifier Agent ---\")\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are an expert CRM Lead Qualifier Agent. Your sole task is to analyze a sales lead \"\n",
        "        \"provided via email address. You must follow these steps precisely: \"\n",
        "        \"1. Identify the domain from the email. \"\n",
        "        \"2. Call `lookup_domain_info` and `check_crm_history` sequentially to gather all data. \"\n",
        "        \"3. Combine all collected data into a single JSON object. \"\n",
        "        \"4. Call `calculate_lead_score` with the combined JSON object. \"\n",
        "        \"5. Finally, synthesize all information (domain info, CRM history, and score) \"\n",
        "        \"into a single, easy-to-read summary for a busy sales rep.\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    collected_data = {}\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n[AI Thinking...]\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            tools=tools_schema,\n",
        "            tool_choice=\"auto\",\n",
        "        )\n",
        "\n",
        "        response_message = response.choices[0].message\n",
        "        messages.append(response_message)\n",
        "\n",
        "        if response_message.tool_calls:\n",
        "            tool_calls = response_message.tool_calls\n",
        "\n",
        "            for tool_call in tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                function_to_call = AVAILABLE_FUNCTIONS.get(function_name)\n",
        "                function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "                if not function_to_call:\n",
        "                    print(f\"Error: Unknown function {function_name}\")\n",
        "                    continue\n",
        "\n",
        "                # Execute the function\n",
        "                function_result = function_to_call(**function_args)\n",
        "\n",
        "                # Update the persistent memory (collected_data)\n",
        "                if function_name == \"lookup_domain_info\":\n",
        "                    collected_data[\"domain_info\"] = json.loads(function_result)\n",
        "                elif function_name == \"check_crm_history\":\n",
        "                    collected_data[\"crm_history\"] = json.loads(function_result)\n",
        "                elif function_name == \"calculate_lead_score\":\n",
        "                    # Inject the accumulated data from previous turns\n",
        "                    function_args = {\"data_summary\": json.dumps(collected_data)}\n",
        "                    function_result = function_to_call(**function_args)\n",
        "\n",
        "                # Append the tool result as a NEW message\n",
        "                messages.append({\n",
        "                    \"role\": \"tool\",\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": function_result\n",
        "                })\n",
        "\n",
        "        else:\n",
        "            print(\"\\n--- FINAL AGENT SUMMARY ---\")\n",
        "            print(response_message.content)\n",
        "            break"
      ],
      "metadata": {
        "id": "kjPA5HySuZqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution and Testing\n",
        "\n",
        "Finally, we test our agent with two distinct scenarios to see how it dynamically adapts its behavior.\n",
        "\n",
        "* **Scenario 1 (Jane @ AcmeCorp)**: Represents a high-value enterprise lead. The agent should detect the high revenue and assign a \"High\" score.\n",
        "* **Scenario 2 (Bob @ WidgetCo)**: Represents a smaller company but with an active deal. The agent should detect the \"Active Opportunity\" status in the CRM and score it accordingly."
      ],
      "metadata": {
        "id": "oS3TFVBu4L5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 1: High-Value Lead (Large company, needs scoring)\n",
        "lead_email_1 = \"jane@acmecorp.com\"\n",
        "run_agent(f\"Please qualify this lead for my call tomorrow: {lead_email_1}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "LWmvdHh1ufg5",
        "outputId": "007080a3-ebfb-48e6-af0d-1f79bdb5d5e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Lead Qualifier Agent ---\n",
            "\n",
            "[AI Thinking...]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'client' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-742559298.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Scenario 1: High-Value Lead (Large company, needs scoring)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlead_email_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"jane@acmecorp.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Please qualify this lead for my call tomorrow: {lead_email_1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1063618432.py\u001b[0m in \u001b[0;36mrun_agent\u001b[0;34m(user_prompt)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n[AI Thinking...]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario 2: Medium-Value Lead (Active opportunity, needs scoring)\n",
        "lead_email_2 = \"bob@widgetco.net\"\n",
        "run_agent(f\"Can you run an analysis on this lead: {lead_email_2}\")"
      ],
      "metadata": {
        "id": "h9n5CzN8xBvh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}